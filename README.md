# Deep-learning-project-for-image-captioning
The project thoroughly explores three different image captioning models: the Convolutional Neural Network (CNN)-Long Short-Term Memory (LSTM) model, the Encoder-Decoder attention model, and the pre-trained transformer-based model.One of the project's objectives is to understand the effect of attention on image captioning and identify the most effective model based on the generated captions. The Flicker8k dataset which comprises 8000 diverse images, each with five captions, is used for training and testing. The result is evaluated using both BLEU (Bilingual evaluation understudy) and METEOR (Metric for Evaluation of Translation with Explicit ORdering) metrics alongside human evaluation, prioritizing the latter.
The result concludes that with a METEOR score of 0.559 and a BLEU score of 0.072, the captions generated by the pre-trained transformer have superior quality, making this model suitable for applications like providing visual assistance to visually impaired individuals, where precise and contextually significant image captions are crucial. The encoder-decoder attention model, with its METEOR score of 0.3669 and BLEU score of 0.0341, with moderately good quality captions has the potential to be used where a reasonable level of caption quality is appropriate. Scoring the lowest values of 0.126 and 1.10e-79, in METEOR and BLEU metrics respectively and generating captions of very low quality, the CNN-LSTM model can be used in simple applications that demand basic object detection from images, such as content labelling


